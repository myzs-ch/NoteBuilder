bs4.BeautifulSoup
requests
jieba  ---中文分词组件
wordcloud  ---词云库

# 1 爬虫需要的模块
requests.get(url,headers):	向网站发起请求，返回响应对象\nurl:需要抓取的url地址\nheaders：请求头\ntimeout：超过时间会抛出异常
响应对象（requests.get()返回值）的属性：	encoding：响应字符编码\ntext:网站源码（字符流）\ncontent:网站源码（字节流）\nstatus_code:HTTP响应码\nurl：实际数据的URL地址

## 1.1 requests
[u4e00- u9fa5] unicode里中文的范围
{{目前最好用的python中文分词组件}}
{{- 支持三种分词模式}}
精确模式：	视图将句子最精确地切开，适合文本分析
全模式：	把句子中所有的可以乘此的词语都扫描出来，速度非常块，但是不能解决歧义
搜索引擎模式：	在精确模式的基础上，对长词再次切分，提高找回率，适合用于搜索引擎分词
jieba.cut(str,cut_all=，HMM=)：	cut_all为True时为全模式，False时为精确模式，返回一个迭代器。可以使用str.join()对返回值进行处理
jieba.cut_for_search(str,HMM:bool=True):	适合用于搜索引擎构建、用倒排索引存储数据，粒度比较细
jieba.lcut()、jieba.lcut_for_search():		在cut的基础上，直接返回一个列表	
HMM（隐马尔科夫模型）：		自然语言处理中的一个基本模型，用于汉语分词、词性标注、语音识别等，在NLP中战友很重要的地位

## 1.2 jieba 
Python Data Analysis Library：	基于NumPy的一种工具，该工具是为了解决数据分析任务而创建。是python 里分析结构化数据的工具集，基础是numpy，图像库是matplotlib。


## 1.3 pandas
